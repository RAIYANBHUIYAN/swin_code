{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceType":"datasetVersion","sourceId":14046428,"datasetId":8942361}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"protobuf==3.20.*\" --force-reinstall\n!pip install -q \"monai[nibabel]\" --no-deps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:12:50.012159Z","iopub.execute_input":"2025-12-07T16:12:50.012675Z","iopub.status.idle":"2025-12-07T16:12:57.934313Z","shell.execute_reply.started":"2025-12-07T16:12:50.012645Z","shell.execute_reply":"2025-12-07T16:12:57.933660Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.*\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, random, numpy as np, nibabel as nib, time\nfrom tqdm.auto import tqdm\nfrom glob import glob\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.transforms import (\n    LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd,\n    CropForegroundd, NormalizeIntensityd, ResizeWithPadOrCropd,\n    ConcatItemsd, EnsureTyped\n)\nfrom monai.data import Dataset, CacheDataset\nfrom monai.networks.nets import SwinUNETR\nfrom monai.losses import DiceCELoss, FocalLoss, TverskyLoss\nfrom monai.inferers import sliding_window_inference\n\nprint(\"MONAI:\", monai.__version__)\nprint(\"Torch:\", torch.__version__)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:12:57.935973Z","iopub.execute_input":"2025-12-07T16:12:57.936217Z","iopub.status.idle":"2025-12-07T16:13:33.813882Z","shell.execute_reply.started":"2025-12-07T16:12:57.936192Z","shell.execute_reply":"2025-12-07T16:13:33.813091Z"}},"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-12-07 16:13:15.332258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765123995.501938      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765123995.551793      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"MONAI: 1.5.1\nTorch: 2.6.0+cu124\nDevice: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"BASE = \"/kaggle/input/brats20-dataset-training-validation\"\n\nTRAIN_DIR = f\"{BASE}/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\nVAL_DIR   = f\"{BASE}/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData\"\nTEST_DIR  = VAL_DIR  # No labels provided, will use for inference only\n\nIMG_SIZE = (96, 96, 96)\nIN_CHANNELS = 4\nOUT_CHANNELS = 4   # 0,1,2,3 (we remap 4->3)\nBATCH_SIZE = 2\nEPOCHS = 10\nLR = 1e-4\n\nOUTDIR = \"/kaggle/working/swin_unetr_run\"\nCKPT_DIR = f\"{OUTDIR}/checkpoints\"; os.makedirs(CKPT_DIR, exist_ok=True)\nPRED_DIR = f\"{OUTDIR}/predictions\"; os.makedirs(PRED_DIR, exist_ok=True)\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\nprint(\"OUTPUT DIR:\", OUTDIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:13:33.814817Z","iopub.execute_input":"2025-12-07T16:13:33.815396Z","iopub.status.idle":"2025-12-07T16:13:33.825966Z","shell.execute_reply.started":"2025-12-07T16:13:33.815376Z","shell.execute_reply":"2025-12-07T16:13:33.825281Z"}},"outputs":[{"name":"stdout","text":"OUTPUT DIR: /kaggle/working/swin_unetr_run\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_brats_train(root):\n    items = []\n    for case in sorted(os.listdir(root)):\n        folder = f\"{root}/{case}\"\n\n        # Skip junk files / CSV / non-directories\n        if not os.path.isdir(folder): \n            continue\n        if case.endswith(\".csv\") or case.startswith(\".\"):\n            continue\n        \n        cid = case\n        \n        f = f\"{folder}/{cid}_flair.nii\"\n        t1 = f\"{folder}/{cid}_t1.nii\"\n        t1ce = f\"{folder}/{cid}_t1ce.nii\"\n        t2 = f\"{folder}/{cid}_t2.nii\"\n        seg = f\"{folder}/{cid}_seg.nii\"\n\n        # Make sure ALL 4 modalities + seg exist\n        if os.path.exists(f) and os.path.exists(t1) and os.path.exists(t1ce) and os.path.exists(t2) and os.path.exists(seg):\n            items.append({\"id\": cid, \"flair\": f, \"t1\": t1, \"t1ce\": t1ce, \"t2\": t2, \"seg\": seg})\n    \n    return items\n\n\ndef load_brats_test(root):\n    items = []\n    for case in sorted(os.listdir(root)):\n        folder = f\"{root}/{case}\"\n        if not os.path.isdir(folder): \n            continue\n        if case.endswith(\".csv\") or case.startswith(\".\"):\n            continue\n\n        cid = case\n        f = f\"{folder}/{cid}_flair.nii\"\n        t1 = f\"{folder}/{cid}_t1.nii\"\n        t1ce = f\"{folder}/{cid}_t1ce.nii\"\n        t2 = f\"{folder}/{cid}_t2.nii\"\n\n        if os.path.exists(f) and os.path.exists(t1) and os.path.exists(t1ce) and os.path.exists(t2):\n            items.append({\"id\": cid, \"flair\": f, \"t1\": t1, \"t1ce\": t1ce, \"t2\": t2})\n    return items\n\n\ntrain_files = load_brats_train(TRAIN_DIR)\ntest_files  = load_brats_test(TEST_DIR)\n\nprint(\"Train cases:\", len(train_files))\nprint(\"Test cases:\", len(test_files))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:13:33.826656Z","iopub.execute_input":"2025-12-07T16:13:33.826884Z","iopub.status.idle":"2025-12-07T16:13:42.852985Z","shell.execute_reply.started":"2025-12-07T16:13:33.826868Z","shell.execute_reply":"2025-12-07T16:13:42.852333Z"}},"outputs":[{"name":"stdout","text":"Train cases: 368\nTest cases: 125\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_transforms = monai.transforms.Compose([\n    LoadImaged(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\"]),\n    EnsureChannelFirstd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\"]),\n    Orientationd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\"], axcodes=\"RAS\"),\n    Spacingd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\"],\n             pixdim=(1,1,1), mode=(\"bilinear\",\"bilinear\",\"bilinear\",\"bilinear\",\"nearest\")),\n    CropForegroundd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\"], source_key=\"flair\"),\n    ResizeWithPadOrCropd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\"], spatial_size=IMG_SIZE),\n    NormalizeIntensityd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], nonzero=True, channel_wise=True),\n    ConcatItemsd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], name=\"image\"),\n    EnsureTyped(keys=[\"image\",\"seg\"])\n])\n\ntest_transforms = monai.transforms.Compose([\n    LoadImaged(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"]),\n    EnsureChannelFirstd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"]),\n    Orientationd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], axcodes=\"RAS\"),\n    Spacingd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], pixdim=(1,1,1),\n             mode=(\"bilinear\",\"bilinear\",\"bilinear\",\"bilinear\")),\n    CropForegroundd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], source_key=\"flair\"),\n    ResizeWithPadOrCropd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], spatial_size=IMG_SIZE),\n    NormalizeIntensityd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], nonzero=True, channel_wise=True),\n    ConcatItemsd(keys=[\"flair\",\"t1\",\"t1ce\",\"t2\"], name=\"image\"),\n    EnsureTyped(keys=[\"image\"])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:13:42.854680Z","iopub.execute_input":"2025-12-07T16:13:42.854920Z","iopub.status.idle":"2025-12-07T16:13:42.868628Z","shell.execute_reply.started":"2025-12-07T16:13:42.854902Z","shell.execute_reply":"2025-12-07T16:13:42.867914Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n  warn_deprecated(argname, msg, warning_category)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_ds = CacheDataset(train_files, transform=train_transforms, cache_rate=0.7)\ntest_ds  = Dataset(test_files, transform=test_transforms)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(train_ds, batch_size=1, shuffle=False)  # since training has labels\ntest_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n\nprint(\"Batches => Train:\", len(train_loader), \"Val:\", len(val_loader), \"Test:\", len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T16:13:42.869338Z","iopub.execute_input":"2025-12-07T16:13:42.869595Z"}},"outputs":[{"name":"stderr","text":"Loading dataset:  19%|â–ˆâ–‰        | 50/257 [02:02<08:58,  2.60s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model = SwinUNETR(\n       \n    in_channels=4,\n    out_channels=4,\n    feature_size=48,\n    use_checkpoint=True\n).to(device)\n\n\nloss_dicece = DiceCELoss(include_background=False, to_onehot_y=True, softmax=True)\nloss_tversky = TverskyLoss(alpha=0.3, beta=0.7, to_onehot_y=True, softmax=True)\nloss_focal = FocalLoss(include_background=False, to_onehot_y=True, gamma=2.0)\n\ndef hybrid_loss(pred, target):\n    target[target == 4] = 3\n    l1 = loss_dicece(pred, target)\n    l2 = loss_tversky(pred, target)\n    l3 = loss_focal(pred, target)\n    return 0.4*l1 + 0.3*l2 + 0.3*l3\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\nscaler = torch.cuda.amp.GradScaler()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_region(pred, gt):\n    pred = pred.cpu().numpy()[0]\n    gt   = gt.cpu().numpy()[0]\n    gt[gt==4] = 3\n\n    eps = 1e-7\n    scores = {}\n    scores[\"WT\"] = (2*np.sum((pred>0)&(gt>0))) / (np.sum(pred>0)+np.sum(gt>0)+eps)\n    scores[\"TC\"] = (2*np.sum(((pred==1)|(pred==3)) & ((gt==1)|(gt==3)))) / (\n                   np.sum((pred==1)|(pred==3))+np.sum((gt==1)|(gt==3))+eps)\n    scores[\"ET\"] = (2*np.sum((pred==3)&(gt==3))) / (np.sum(pred==3)+np.sum(gt==3)+eps)\n    return scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ckpt = torch.load(f\"{CKPT_DIR}/best_swin_hybrid.pth\", map_location=device)\nmodel.load_state_dict(ckpt[\"model_state\"])\noptimizer.load_state_dict(ckpt[\"optimizer_state\"])\nstart_epoch = ckpt[\"epoch\"] + 1\nprint(\"ğŸ”„ Resuming Training from Epoch\", start_epoch)\nfor epoch in range(start_epoch, EPOCHS+1):\n","metadata":{}},{"cell_type":"code","source":"best_dice = 0\nper_class_dice_history = []   \ntrain_losses = []             \n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    running_loss = 0\n\n    # ================== TRAIN ==================\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n    for batch in pbar:\n        img = batch[\"image\"].to(device)\n        seg = batch[\"seg\"].to(device)\n\n        optimizer.zero_grad()\n        with torch.amp.autocast('cuda'):\n            pred = model(img)\n            loss = hybrid_loss(pred, seg)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        pbar.set_postfix({\"loss\": f\"{running_loss/(pbar.n+1):.4f}\"})\n\n    # ================== VALIDATION ==================\n    model.eval()\n    dices = {\"WT\":[], \"TC\":[], \"ET\":[]}\n\n    with torch.no_grad():\n        vbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n        for v in vbar:\n            img = v[\"image\"].to(device)\n            seg = v[\"seg\"].to(device)\n\n            out = sliding_window_inference(img, IMG_SIZE, 1, model)\n            pred = torch.argmax(out.softmax(1), 1, keepdim=True)\n            d = dice_region(pred, seg)\n            for k in d:\n                dices[k].append(d[k])\n\n    mean_dice = np.mean([np.mean(v) for v in dices.values()])\n    print(f\"ğŸ¯ Dice -> WT:{np.mean(dices['WT']):.4f}, TC:{np.mean(dices['TC']):.4f}, ET:{np.mean(dices['ET']):.4f}, Mean:{mean_dice:.4f}\")\n\n    # ---- Save epoch metrics ----\n    per_class_dice_history.append([\n        np.mean(dices[\"WT\"]),\n        np.mean(dices[\"TC\"]),\n        np.mean(dices[\"ET\"])\n    ])\n    train_losses.append(running_loss/(len(train_loader)))\n\n    # ---- Save BEST Model with optimizer ----\n    if mean_dice > best_dice:\n        best_dice = mean_dice\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"best_dice\": best_dice\n        }, f\"{CKPT_DIR}/best_swin_hybrid.pth\")\n        print(\"ğŸ’¾ Saved BEST model (with optimizer)!\")\n\n# ================== SAVE LAST EPOCH MODEL ==================\ntorch.save(model.state_dict(), f\"{CKPT_DIR}/last_epoch_swin_hybrid.pth\")\nprint(\"ğŸ“Œ Saved LAST epoch model!\")\n\n# ================== SAVE TRAINING METRICS ==================\nimport pandas as pd\n\ndf = pd.DataFrame(per_class_dice_history, columns=[\"WT\",\"TC\",\"ET\"])\ndf[\"Mean\"] = df.mean(axis=1)\ndf[\"Train_Loss\"] = train_losses\ndf.to_csv(f\"{OUTDIR}/metrics_log.csv\", index_label=\"Epoch\")\n\nprint(\"ğŸ“Š Metrics saved to:\", f\"{OUTDIR}/metrics_log.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Define device again (if needed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define checkpoint directory\nCKPT_DIR = \"/kaggle/working/swin_unetr_run\"\n\n# ===== LOAD BEST CHECKPOINT =====\nckpt = torch.load(f\"{CKPT_DIR}/best_swin_hybrid.pth\", map_location=device)\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.to(device)\nmodel.eval()\n\nprint(\"âœ” Loaded Trained SwinUNETR Model!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch.serialization as serialization\n\n# allow numpy scalar (needed in PyTorch 2.6+)\nserialization.add_safe_globals([np.generic])\n\n# Load checkpoint safely\nckpt = torch.load(\n    f\"{CKPT_DIR}/best_swin_hybrid.pth\", \n    map_location=device, \n    weights_only=False\n)\n\n# Restore model\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch in tqdm(test_loader, desc=\"Inference\"):\n    pid = batch[\"id\"][0]\n    img = batch[\"image\"].to(device)\n\n    out = sliding_window_inference(img, IMG_SIZE, 1, model)\n    pred = torch.argmax(out.softmax(1), 1).cpu().numpy()[0]\n\n    affine = np.eye(4)\n    for key in [\"flair\",\"t1\",\"t1ce\",\"t2\"]:\n        if key in batch and hasattr(batch[key][0], \"meta\"):\n            try:\n                affine = nib.load(batch[key][0].meta[\"filename_or_obj\"]).affine\n                break\n            except:\n                pass\n\n    nib.save(\n        nib.Nifti1Image(pred.astype(np.uint8), affine),\n        f\"{PRED_DIR}/{pid}_pred.nii.gz\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# ===================== SAVE RESULTS =====================\ndf = pd.DataFrame(per_class_dice_history, columns=[\"WT\",\"TC\",\"ET\"])\ndf[\"Mean\"] = df.mean(axis=1)\ndf[\"Train_Loss\"] = train_losses\ndf.to_csv(f\"{OUTDIR}/metrics_log.csv\", index_label=\"Epoch\")\n\nprint(\"\\nğŸ“Œ Metrics saved at:\", f\"{OUTDIR}/metrics_log.csv\")\n\n# ===================== PLOT DICE CURVES =====================\nplt.figure(figsize=(10,5))\nplt.plot(df[\"WT\"], label=\"WT\")\nplt.plot(df[\"TC\"], label=\"TC\")\nplt.plot(df[\"ET\"], label=\"ET\")\nplt.plot(df[\"Mean\"], label=\"Mean\", linewidth=3, color=\"black\")\nplt.title(\"Dice Score Progress\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Dice\")\nplt.legend()\nplt.grid()\nplt.savefig(f\"{OUTDIR}/dice_curve.png\", dpi=200)\nplt.show()\n\n# ===================== PLOT LOSS =====================\nplt.figure(figsize=(6,4))\nplt.plot(df[\"Train_Loss\"], marker=\"o\")\nplt.title(\"Training Loss Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid()\nplt.savefig(f\"{OUTDIR}/loss_curve.png\", dpi=200)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# --------------------------\n# Utility for Dice\n# --------------------------\ndef dice_coef(pred, gt, cls):\n    pred_c = (pred == cls)\n    gt_c   = (gt == cls)\n    if gt_c.sum() == 0 and pred_c.sum() == 0:\n        return 1.0\n    return (2 * np.logical_and(pred_c, gt_c).sum()) / (pred_c.sum() + gt_c.sum() + 1e-8)\n\n# --------------------------\n# PICK SAMPLE\n# --------------------------\nsample = next(iter(test_loader))\nimg = sample[\"image\"].to(device)\n\nwith torch.no_grad():\n    out = sliding_window_inference(img, IMG_SIZE, 1, model)\npred = torch.argmax(out.softmax(1), 1).cpu().numpy()[0]\n\ngt = sample[\"seg\"][0,0].cpu().numpy()\nslice_id = np.argmax((gt > 0).sum(axis=(1,2)))  # slice with max tumor\n\nmri_slice = img[0,0,slice_id].cpu().numpy()\npred_slice = pred[slice_id]\ngt_slice = gt[slice_id]\nerror_map = np.abs(gt_slice - pred_slice)\n\n# --------------------------\n# DICE SCORES\n# --------------------------\ndice_ET = dice_coef(pred, gt, 3)\ndice_TC = dice_coef(pred, gt, 2)\ndice_WT = dice_coef(pred, gt, 1)\ndice_BG = dice_coef(pred, gt, 0)\n\n# --------------------------\n# PLOT\n# --------------------------\nplt.figure(figsize=(16,12))\n\n# 1) MRI\nplt.subplot(3,4,1)\nplt.title(\"Original MRI Image\")\nplt.imshow(mri_slice, cmap='gray')\nplt.axis('off')\n\n# 2) Ground truth\nplt.subplot(3,4,2)\nplt.title(\"Ground Truth Mask\")\nplt.imshow(gt_slice, cmap='tab20')\nplt.axis('off')\n\n# 3) Prediction\nplt.subplot(3,4,3)\nplt.title(\"Predicted Mask\")\nplt.imshow(pred_slice, cmap='tab20')\nplt.axis('off')\n\n# 4) Error Map\nplt.subplot(3,4,4)\nplt.title(\"Error Map (|GT - Pred|)\")\nplt.imshow(error_map, cmap='Reds')\nplt.colorbar(label=\"Error Magnitude\")\nplt.axis('off')\n\n# --------------------------\n# Per-Class Overlays\n# --------------------------\ndef masked_overlay(img, mask, cls, title):\n    base = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n    base = np.uint8(base); base = np.stack([base]*3, axis=-1)\n    m = np.uint8(mask == cls)\n    m = cv2.resize(m, (base.shape[1], base.shape[0]))\n    color = np.zeros_like(base)\n    if cls == 3:  color[:] = (255, 0, 0)     # ET = Red\n    if cls == 2:  color[:] = (0, 255, 0)     # TC = Green\n    if cls == 1:  color[:] = (0, 0, 255)     # WT = Blue\n    overlay = cv2.addWeighted(base, 0.6, color, 0.4, 0)\n    plt.imshow(np.where(m[...,None]==1, overlay, base))\n    plt.title(title)\n    plt.axis('off')\n\n# ET\nplt.subplot(3,4,5)\nmasked_overlay(mri_slice, gt_slice, 3, f\"ET (Enhancing Tumor)\\nDice: {dice_ET:.3f}\")\n\n# TC\nplt.subplot(3,4,6)\nmasked_overlay(mri_slice, gt_slice, 2, f\"TC (Tumor Core)\\nDice: {dice_TC:.3f}\")\n\n# WT\nplt.subplot(3,4,7)\nmasked_overlay(mri_slice, gt_slice, 1, f\"WT (Whole Tumor)\\nDice: {dice_WT:.3f}\")\n\n# Background\nplt.subplot(3,4,8)\nmasked_overlay(mri_slice, gt_slice, 0, f\"Background\\nDice: {dice_BG:.3f}\")\n\n# --------------------------\n# TEXT BOX SUMMARY\n# --------------------------\nplt.subplot(3,4,9)\nplt.axis('off')\ntext = f\"\"\"\nğŸ“Œ Sample Metrics Summary:\n\nğŸ”¹ Region Dice Scores:\n   âœ“ ET (Enhancing): {dice_ET:.3f}\n   âœ“ TC (Tumor Core): {dice_TC:.3f}\n   âœ“ WT (Whole Tumor): {dice_WT:.3f}\n   âœ“ Background: {dice_BG:.3f}\n\nğŸ”¸ Clinical Acceptance:\n   ET > 0.78  â†’ {'âš ï¸ FAIL' if dice_ET<0.78 else 'âœ… PASS'}\n   TC > 0.70  â†’ {'âš ï¸ FAIL' if dice_TC<0.70 else 'âœ… PASS'}\n   WT > 0.85  â†’ {'âš ï¸ FAIL' if dice_WT<0.85 else 'âœ… PASS'}\n\"\"\"\nplt.text(0.01, 0.5, text, fontsize=10, va='center', bbox=dict(facecolor='#D4EFFC', edgecolor='black'))\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GradCam\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# Choose a layer from SwinViT backbone\ntarget_layer = model.swinViT.layers2[0].blocks[-1].norm1\n\n# Containers\ngradients = []\nactivations = []\n\n# Save forward activations\ndef save_activation(module, inp, out):\n    activations.append(out)\n\n# Save backward gradients\ndef save_gradient(module, grad_in, grad_out):\n    gradients.append(grad_out[0])\n\n# Register hooks\ntarget_layer.register_forward_hook(save_activation)\ntarget_layer.register_full_backward_hook(save_gradient)\nprint(\"ğŸ¯ GradCAM hooks attached to:\", target_layer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"act = activations[-1]\ngrad = gradients[-1]\n\n# Ensure batch dimension removed\nif act.dim() == 5:  # (B, C, D, H, W)\n    act = act[0]\n    grad = grad[0]\n\nact = act.detach().cpu().numpy()\ngrad = grad.detach().cpu().numpy()\n\n# Compute channel weights (average gradient per channel)\nweights = grad.mean(axis=(1,2,3), keepdims=True)  # collapse D,H,W\n\n# Weighted sum of activations\ncam = np.maximum((weights * act).sum(axis=0), 0)\n\n# Normalize heatmap\ncam = cam / (cam.max() + 1e-8)\n\nprint(\"ğŸ”¥ CAM ready, shape:\", cam.shape)  # Should be (D,H,W)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmid = cam.shape[0] // 2  # middle slice\n\nplt.figure(figsize=(6,6))\nplt.imshow(cam[mid], cmap='jet')\nplt.colorbar()\nplt.title(\"GradCAM Heatmap (Mid Slice)\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================\n# ğŸ“Œ GRADCAM + MULTI-CLASS EXPLAINABILITY (WT-TC-ET)\n# ===============================================\n\nimport torch, numpy as np, cv2, matplotlib.pyplot as plt\n\n# -----------------------------------------------\n# ğŸ”— Attach Grad-CAM Hooks (do this ONCE)\n# -----------------------------------------------\ntarget_layer = model.swinViT.layers2[0].blocks[-1].norm1  # Good mid-depth CAM layer\n\ngradients = []\nactivations = []\n\ndef save_activation(module, inp, out):\n    activations.append(out)\n\ndef save_gradient(module, grad_in, grad_out):\n    gradients.append(grad_out[0])\n\ntarget_layer.register_forward_hook(save_activation)\ntarget_layer.register_full_backward_hook(save_gradient)\n\nprint(\"ğŸ¯ GradCAM attached to:\", target_layer)\n\n# -----------------------------------------------------\n# ğŸ§  Function to generate CAM for a given class label\n#   Class indices:\n#      1 = WT (Whole Tumor)\n#      2 = TC (Tumor Core)\n#      3 = ET (Enhancing Tumor)\n# -----------------------------------------------------\ndef build_cam(prob_output, cls_index):\n    model.zero_grad()\n    score = prob_output[cls_index].mean()\n    score.backward(retain_graph=True)\n\n    act = activations[-1]\n    grad = gradients[-1]\n\n    # Remove batch if present\n    if act.dim() == 5:\n        act = act[0]\n        grad = grad[0]\n\n    act = act.detach().cpu().numpy()\n    grad = grad.detach().cpu().numpy()\n\n    weights = grad.mean(axis=(1,2,3), keepdims=True)  # average gradient per channel\n    cam = np.maximum((weights * act).sum(axis=0), 0)  # weighted sum\n    cam = cam / (cam.max() + 1e-8)  # normalize 0-1\n    return cam  # shape: (D, H, W)\n\n# -----------------------------------------------------\n# ğŸ¨ Overlay utility: CAM + MRI Slice\n# -----------------------------------------------------\ndef overlay(cam, base_img):\n    base = cv2.normalize(base_img, None, 0, 255, cv2.NORM_MINMAX)\n    base = np.uint8(base)\n\n    cam_resized = cv2.resize(cam, (base.shape[1], base.shape[0]))\n\n    heat = cv2.applyColorMap(np.uint8(cam_resized * 255), cv2.COLORMAP_JET)\n    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)\n\n    base_rgb = np.stack([base]*3, axis=-1)\n    return cv2.addWeighted(base_rgb, 0.6, heat, 0.4, 0)\n\n# -----------------------------------------------------\n# ğŸ§ª Generate CAM for a sample from Validation\n# -----------------------------------------------------\nsample = next(iter(val_loader))\nimg = sample[\"image\"].to(device)\ngt = sample[\"seg\"][0,0].cpu().numpy()  # ground truth segmentation\n\nmodel.eval()\noutput = model(img)\nprob = output.softmax(1)[0]  # class probabilities\n\n# Compute CAMs for WT, TC, ET\ncam_WT = build_cam(prob, 1)\ncam_TC = build_cam(prob, 2)\ncam_ET = build_cam(prob, 3)\n\n# Pick middle slice\nslice_id = cam_WT.shape[0] // 2\nmri_slice = img[0,0, slice_id].detach().cpu().numpy()  # FLAIR view for visualization\npred_slice = torch.argmax(prob, 0).cpu().numpy()[slice_id]\n\n# Overlay CAMs on MRI\nov_WT = overlay(cam_WT[slice_id], mri_slice)\nov_TC = overlay(cam_TC[slice_id], mri_slice)\nov_ET = overlay(cam_ET[slice_id], mri_slice)\n\n# -----------------------------------------------------\n# ğŸ“Œ DISPLAY ALL VISUALS\n# -----------------------------------------------------\nplt.figure(figsize=(14,10))\n\nplt.subplot(2,3,1); plt.title(\"MRI Slice\"); plt.imshow(mri_slice, cmap='gray'); plt.axis('off')\nplt.subplot(2,3,2); plt.title(\"Ground Truth\"); plt.imshow(gt[slice_id], cmap='tab20'); plt.axis('off')\nplt.subplot(2,3,3); plt.title(\"Prediction\"); plt.imshow(pred_slice, cmap='tab20'); plt.axis('off')\n\nplt.subplot(2,3,4); plt.title(\"WT CAM Overlay\"); plt.imshow(ov_WT); plt.axis('off')\nplt.subplot(2,3,5); plt.title(\"TC CAM Overlay\"); plt.imshow(ov_TC); plt.axis('off')\nplt.subplot(2,3,6); plt.title(\"ET CAM Overlay\"); plt.imshow(ov_ET); plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}